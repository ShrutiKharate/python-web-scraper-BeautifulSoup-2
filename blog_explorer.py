# -*- coding: utf-8 -*-
"""blog_explorer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ix_VDWa3qaWXvwJpWOtvQYFdQoUnlMEB
"""

# Import the necessary libraries
import argparse
import requests
from bs4 import BeautifulSoup
import shlex

def main():
    """The main function where our script's logic lives."""
    # Set up argparse to handle the blog URL and number of articles
    parser = argparse.ArgumentParser(description="Scrape the latest articles from a tech blog.")
    parser.add_argument('-u', '--url', required=True, help='The URL of the blog feed or main page.')
    parser.add_argument('-n', '--num_articles', type=int, default=5, help='Number of articles to extract.')

    # --- Google Colab Workaround ---
    # We simulate the command-line arguments here.
    cmd_line_input = 'blog_explorer.py -u "https://dev.to" -n 7'
    args = parser.parse_args(shlex.split(cmd_line_input)[1:])
    # --- End of Workaround ---

    url = args.url
    num_articles = args.num_articles

    print(f"Fetching the latest {num_articles} articles from {url}\n")

    # This header makes our script look like a real browser
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }

    try:
        # Make the request, including our custom header
        response = requests.get(url, headers=headers)
        response.raise_for_status()

        # Parse the page content with BeautifulSoup
        soup = BeautifulSoup(response.content, 'html.parser')

        # --- UPDATED HTML SELECTOR ---
        # After re-inspecting dev.to, we found that articles are now
        # inside a <div> with the class 'crayons-story'.
        articles = soup.find_all('div', class_='crayons-story', limit=num_articles)

        if not articles:
            print("Could not find any articles. The website's HTML structure may have changed, or the new selectors are incorrect.")
            return

        print(f"--- Found {len(articles)} Articles ---")
        # Loop through each article container we found
        for i, article in enumerate(articles):
            # The title selector still works! It's an h2 with this class.
            title_element = article.find('h2', class_='crayons-story__title')

            if title_element and title_element.find('a'):
                link_tag = title_element.find('a')

                title = link_tag.text.strip()
                link_href = link_tag['href']

                # --- FIX for duplicated URLs ---
                # Check if the link is already a full URL or a relative one
                if link_href.startswith('http'):
                    full_url = link_href
                else:
                    full_url = "https://dev.to" + link_href

                print(f"{i+1}. {title}")
                print(f"   URL: {full_url}\n")
            else:
                print(f"{i+1}. Could not extract title for an article within a 'crayons-story' div.\n")

    except requests.exceptions.RequestException as e:
        print(f"An error occurred while fetching the URL: {e}")

# Run the main function when the script is executed
if __name__ == '__main__':
    main()

